/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: please do not edit it. Instead, edit the
// BAML files and re-generate this code using: baml-cli generate
// You can install baml-cli with:
//  $ npm install @boundaryml/baml
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code

const fileMap = {
  
  "chatbot.baml": "// Conversation history for context\r\nclass ConversationHistory {\r\n  messages string[] @description(\"List of previous messages in the conversation\")\r\n}\r\n\r\n// Simple response with just the answer\r\nclass ChatResponse {\r\n  answer string @description(\"The chatbot's answer to the user's question\")\r\n}\r\n\r\n// Non-streaming chat function\r\nfunction Chat(\r\n  user_question: string,\r\n  conversation_history: ConversationHistory?\r\n) -> ChatResponse {\r\n  client AzureOpenAI\r\n\r\n  prompt #\"\r\n    {{ _.role(\"system\") }}\r\n    You are a helpful AI assistant. Answer questions clearly and concisely.\r\n\r\n    {% if conversation_history %}\r\n    ## Previous Conversation:\r\n    {% for msg in conversation_history.messages %}\r\n    {{ msg }}\r\n    {% endfor %}\r\n    {% endif %}\r\n\r\n    {{ _.role(\"user\") }}\r\n    {{ user_question }}\r\n\r\n    {{ ctx.output_format }}\r\n  \"#\r\n}\r\n\r\n// Streaming chat function for real-time responses\r\nfunction StreamChat(\r\n  user_question: string,\r\n  conversation_history: ConversationHistory?\r\n) -> ChatResponse {\r\n  client AzureOpenAI\r\n\r\n  prompt #\"\r\n    {{ _.role(\"system\") }}\r\n    You are a helpful AI assistant. Answer questions clearly and concisely.\r\n\r\n    {% if conversation_history %}\r\n    ## Previous Conversation:\r\n    {% for msg in conversation_history.messages %}\r\n    {{ msg }}\r\n    {% endfor %}\r\n    {% endif %}\r\n\r\n    {{ _.role(\"user\") }}\r\n    {{ user_question }}\r\n\r\n    {{ ctx.output_format }}\r\n  \"#\r\n}\r\n",
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\r\n\r\n// Using the new OpenAI Responses API for enhanced formatting\r\nclient<llm> CustomGPT5 {\r\n  provider openai\r\n  options {\r\n    model \"gpt-5.2\"\r\n    api_key env.OPENAI_API_KEY\r\n  }\r\n}\r\n\r\nclient<llm> Gemini {\r\n  provider google-ai\r\n  options {\r\n    model \"gemini-3-pro-preview\"\r\n    api_key env.GOOGLE_API_KEY\r\n    generationConfig {\r\n      maxOutputTokens 16000\r\n    }\r\n  }\r\n}\r\n\r\n// Claude Opus 4.5 - Most capable model (released Nov 2025)\r\nclient<llm> CustomOpus45 {\r\n  provider anthropic\r\n  options {\r\n    model \"claude-opus-4-5-20251101\"\r\n    api_key env.ANTHROPIC_API_KEY\r\n    max_tokens 16000\r\n  }\r\n}\r\n\r\n// Claude Sonnet 4.5 - Best coding model and strongest for complex agents (released Sep 2025)\r\nclient<llm> CustomSonnet45 {\r\n  provider anthropic\r\n  options {\r\n    model \"claude-sonnet-4-5-20250929\"\r\n    api_key env.ANTHROPIC_API_KEY\r\n    max_tokens 16000\r\n  }\r\n}\r\n\r\nclient<llm> AzureOpenAI {\r\n  provider azure-openai\r\n  options {\r\n    base_url env.AZURE_OPENAI_BASE_URL\r\n    api_version env.AZURE_OPENAI_API_VERSION\r\n    api_key env.AZURE_OPENAI_API_KEY\r\n    max_tokens null\r\n  }\r\n}\r\n\r\n// Main Ollama client for Briscola\r\n// Uses OLLAMA_BASE_URL env var in Docker, defaults to localhost\r\nclient<llm> Ollama {\r\n  provider \"openai-generic\"\r\n  retry_policy Constant\r\n  options {\r\n    base_url env.OLLAMA_BASE_URL ?? \"http://localhost:11434/v1\"\r\n    model qwen3:4b\r\n    // Optimize for speed\r\n    temperature 0.3\r\n    max_tokens 100\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\r\nclient<llm> OpenaiFallback {\r\n  provider fallback\r\n  options {\r\n    // This will try the clients in order until one succeeds\r\n    strategy [AzureOpenAI, CustomGPT5]\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/retry\r\nretry_policy Constant {\r\n  max_retries 3\r\n  strategy {\r\n    type constant_delay\r\n    delay_ms 200\r\n  }\r\n}\r\n\r\nretry_policy Exponential {\r\n  max_retries 2\r\n  strategy {\r\n    type exponential_backoff\r\n    delay_ms 300\r\n    multiplier 1.5\r\n    max_delay_ms 10000\r\n  }\r\n}\r\n",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\r\n// your choice. You can have multiple generators if you use multiple languages.\r\n// Just ensure that the output_dir is different for each generator.\r\ngenerator target {\r\n    // Valid values: \"python/pydantic\", \"typescript\", \"go\", \"rust\", \"ruby/sorbet\", \"rest/openapi\"\r\n    output_type \"typescript\"\r\n\r\n    // Where the generated code will be saved (relative to baml_src/)\r\n    output_dir \"../\"\r\n\r\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\r\n    // The BAML VSCode extension version should also match this version.\r\n    version \"0.219.0\"\r\n\r\n    // Valid values: \"sync\", \"async\"\r\n    // This controls what `b.FunctionName()` will be (sync or async).\r\n    default_client_mode async\r\n}\r\n",
}
export const getBamlFiles = () => {
    return fileMap;
}